import pendulum
import os
import re # ì •ê·œí‘œí˜„ì‹ ì„í¬íŠ¸
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.email import EmailOperator
from airflow.utils.task_group import TaskGroup
from dags.scripts.generate_pdf_report import generate_pdf
from dags.scripts.move_json_files import move_json_files

local_tz = pendulum.timezone("Asia/Seoul")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='news_data_pipeline_unified',
    default_args=default_args,
    description='í†µí•© ë‰´ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸: ì‹¤ì‹œê°„ ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±',
    schedule_interval='@daily',  # ë§¤ì¼ ì‹¤í–‰
    start_date=datetime(2025, 5, 1, tzinfo=local_tz),
    catchup=False,
    tags=['news', 'data_pipeline', 'realtime', 'hdfs', 'spark', 'report'],
    doc_md="""
    ### í†µí•© ë‰´ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ DAG

    ì´ DAGëŠ” ë‰´ìŠ¤ ë°ì´í„°ì˜ ì „ì²´ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤:

    **1. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ê·¸ë£¹:**
    - Kafka Producer: RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ë° Kafka ì „ì†¡ (ë°œí–‰ ë©”ì‹œì§€ ê°œìˆ˜ XCom ì „ë‹¬)
    - Flink Consumer: Kafka â†’ AI ì²˜ë¦¬ â†’ PostgreSQL & HDFS ì €ì¥ (XCom ë©”ì‹œì§€ ê°œìˆ˜ ê¸°ë°˜ ì¢…ë£Œ)

    **2. ë°°ì¹˜ ë¶„ì„ ë° ë¦¬í¬íŒ… ê·¸ë£¹:**
    - HDFS ìƒíƒœ í™•ì¸
    - Spark ë°°ì¹˜ ì‘ì—…: HDFS ë°ì´í„° ë¶„ì„
    - PDF ë¦¬í¬íŠ¸ ìƒì„±
    - íŒŒì¼ ì•„ì¹´ì´ë¸Œ
    - ì´ë©”ì¼ ë¦¬í¬íŠ¸ ì „ì†¡

    **ë°ì´í„° íë¦„:**
    RSS â†’ Kafka â†’ Flink (AI ì²˜ë¦¬) â†’ HDFS â†’ Spark â†’ PDF â†’ Email
    """,
) as dag:

    # ========================================
    # 1. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ TaskGroup
    # ========================================
    with TaskGroup(
        group_id='realtime_data_ingestion',
        tooltip='ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬'
    ) as realtime_group:

        # Kafka Producer ì‹¤í–‰ íƒœìŠ¤í¬
        # í‘œì¤€ ì¶œë ¥ì—ì„œ "Total messages produced: [N]" íŒ¨í„´ì„ ì°¾ì•„ XComìœ¼ë¡œ í‘¸ì‹œ
        run_kafka_producer_task = BashOperator(
            task_id='run_kafka_producer',
            bash_command='''
            python_output=$(docker exec kafka python /opt/workspace/kafka_producer/news_producer_with_XCOM.py 2>&1)
            echo "$python_output"
            # ë§ˆì§€ë§‰ ë¼ì¸ì—ì„œ ë©”ì‹œì§€ ê°œìˆ˜ ì¶”ì¶œ
            MESSAGE_COUNT=$(echo "$python_output" | tail -n 10 | grep -oP 'Total messages produced: \K\d+' | tail -1)
            echo "Extracted MESSAGE_COUNT: $MESSAGE_COUNT"
            # Airflow XComìœ¼ë¡œ í‘¸ì‹œ (task_idëŠ” run_kafka_producer_task)
            echo "$MESSAGE_COUNT"
            ''',
            do_xcom_push=True, # í‘œì¤€ ì¶œë ¥ì˜ ë§ˆì§€ë§‰ ë¼ì¸ì„ XComìœ¼ë¡œ í‘¸ì‹œ
            doc_md="""
            ### Kafka Producer
            - RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
            - í¬ë¡¤ë§ì„ í†µí•œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            - Kafka í† í”½ 'news'ë¡œ ë°ì´í„° ì „ì†¡
            - **ì „ì†¡ëœ ë©”ì‹œì§€ ê°œìˆ˜ë¥¼ XComìœ¼ë¡œ ì „ë‹¬**
            """
        )

        # Flink Consumer ì‹¤í–‰ íƒœìŠ¤í¬
        # Kafka Producerì—ì„œ XComìœ¼ë¡œ ë°›ì€ ë©”ì‹œì§€ ê°œìˆ˜ë¥¼ Flink Consumerì— ì¸ìë¡œ ì „ë‹¬
        run_flink_consumer_task = BashOperator(
            task_id='run_flink_consumer',
            bash_command='''
            MESSAGES_TO_PROCESS="{{ task_instance.xcom_pull(task_ids='realtime_data_ingestion.run_kafka_producer', key='return_value') }}"
            echo "Flink Consumerê°€ ì²˜ë¦¬í•  ë©”ì‹œì§€ ê°œìˆ˜: $MESSAGES_TO_PROCESS"
            # Flink ì»¨ìŠˆë¨¸ ìŠ¤í¬ë¦½íŠ¸ì— ë©”ì‹œì§€ ê°œìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬
            docker exec flink python /opt/workspace/flink_consumer/flink_consumer_with_XCOM.py "$MESSAGES_TO_PROCESS"
            ''',
            do_xcom_push=False,
            doc_md="""
            ### Flink Consumer
            - Kafkaì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì†Œë¹„
            - AI ê¸°ë°˜ ì „ì²˜ë¦¬ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œ, ì„ë² ë”©)
            - PostgreSQL ë° HDFS ë™ì‹œ ì €ì¥
            - **Kafka Producerê°€ ì „ë‹¬í•œ ë©”ì‹œì§€ ê°œìˆ˜ë§Œí¼ ì²˜ë¦¬ í›„ ìë™ ì¢…ë£Œ**
            """
        )

        # ì‹¤ì‹œê°„ ê·¸ë£¹ ë‚´ ì˜ì¡´ì„±
        run_kafka_producer_task >> run_flink_consumer_task

    # ========================================
    # 2. ë°°ì¹˜ ë¶„ì„ ë° ë¦¬í¬íŒ… TaskGroup
    # ========================================
    with TaskGroup(
        group_id='batch_analysis_reporting',
        tooltip='ë°°ì¹˜ ë¶„ì„ ë° ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±'
    ) as batch_group:

        # HDFS ìƒíƒœ í™•ì¸ íƒœìŠ¤í¬
        check_hdfs_status = BashOperator(
            task_id='check_hdfs_status',
            bash_command='''
            echo "ğŸ“Š HDFS ìƒíƒœ í™•ì¸ ì¤‘..."
            echo "=== NameNode ìƒíƒœ ==="
            curl -s "http://namenode:9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus" | head -n 5 || echo "âš ï¸ HDFS ìƒíƒœ í™•ì¸ ì‹¤íŒ¨"

            echo "=== Realtime ë””ë ‰í† ë¦¬ íŒŒì¼ ìˆ˜ ==="
            REALTIME_COUNT=$(curl -s "http://namenode:9870/webhdfs/v1/user/realtime/?op=LISTSTATUS" | grep -o '"pathSuffix":"[^"]*"' | wc -l || echo "0")
            echo "ğŸ“ /user/realtime/ íŒŒì¼ ìˆ˜: $REALTIME_COUNT"

            echo "=== Archive ë””ë ‰í† ë¦¬ ìƒíƒœ ==="
            ARCHIVE_COUNT=$(curl -s "http://namenode:9870/webhdfs/v1/user/news_archive/?op=LISTSTATUS" | grep -o '"pathSuffix":"[^"]*"' | wc -l || echo "0")
            echo "ğŸ“ /user/news_archive/ ë””ë ‰í† ë¦¬ ìˆ˜: $ARCHIVE_COUNT"

            echo "âœ… HDFS ìƒíƒœ í™•ì¸ ì™„ë£Œ"
            ''',
            doc_md="""
            ### HDFS ìƒíƒœ í™•ì¸
            - NameNode í—¬ìŠ¤ ì²´í¬
            - /user/realtime/ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ìˆ˜ í™•ì¸
            - /user/news_archive/ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
            """
        )

        # Spark ì‘ì—…ìœ¼ë¡œ HDFSì—ì„œ ë°ì´í„° ì½ì–´ì„œ ë¦¬í¬íŠ¸ ìƒì„±
        submit_spark_job = SparkSubmitOperator(
            task_id='spark_daily_report_hdfs',
            application='/opt/airflow/dags/scripts/spark_daily_report.py',
            conn_id='spark_default',
            application_args=['--date', '{{ ds }}'],
            conf={
                'spark.master': 'spark://spark-master:7077',
                'spark.sql.adaptive.enabled': 'true',
                'spark.sql.adaptive.coalescePartitions.enabled': 'true',
                'spark.sql.catalog.spark_catalog.type': 'hadoop',
                'spark.hadoop.fs.defaultFS': 'hdfs://namenode:9000'
            },
            doc_md="""
            ### Spark ì¼ì¼ ë¶„ì„ ì‘ì—…
            - HDFS /user/realtime/ ë””ë ‰í† ë¦¬ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì½ê¸°
            - ì¹´í…Œê³ ë¦¬ë³„, ì†ŒìŠ¤ë³„, ì‹œê°„ëŒ€ë³„ ë¶„ì„
            - í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
            - ë¶„ì„ ê²°ê³¼ë¥¼ /opt/airflow/output/daily_reports/ì— ì €ì¥
            """
        )

        # PDF ë¦¬í¬íŠ¸ ìƒì„±
        make_pdf_reports = PythonOperator(
            task_id='make_pdf_reports',
            python_callable=generate_pdf,
            op_kwargs={
                'date': '{{ ds }}',
                'input_dir': '/opt/airflow/output/daily_reports',
                'output_dir': '/opt/airflow/output/daily_reports'
            },
            doc_md="""
            ### PDF ë¦¬í¬íŠ¸ ìƒì„±
            - Spark ë¶„ì„ ê²°ê³¼ë¥¼ PDF í˜•íƒœë¡œ ë³€í™˜
            - ì°¨íŠ¸ ë° ê·¸ë˜í”„ í¬í•¨
            - ì´ë©”ì¼ ì²¨ë¶€ìš© ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            """
        )

        # HDFS ë‚´ì—ì„œ íŒŒì¼ ì´ë™ (realtime â†’ news_archive)
        move_files_to_archive = PythonOperator(
            task_id='move_json_files_hdfs',
            python_callable=move_json_files,
            provide_context=True,
            doc_md="""
            ### íŒŒì¼ ì•„ì¹´ì´ë¸Œ
            - /user/realtime/ â†’ /user/news_archive/{{ ds }}/ë¡œ íŒŒì¼ ì´ë™
            - ì²˜ë¦¬ ì™„ë£Œëœ ë‰´ìŠ¤ ë°ì´í„° ì •ë¦¬
            - HDFS ìŠ¤í† ë¦¬ì§€ íš¨ìœ¨ì„± í–¥ìƒ
            """
        )

        # ì´ë©”ì¼ ë¦¬í¬íŠ¸ ì „ì†¡
        send_email_report = EmailOperator(
            task_id='send_email_report',
            to='wndus51445@gmail.com',
            subject='ğŸ“° [ë‰´ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸] {{ ds }} ì¼ì¼ ë¦¬í¬íŠ¸',
            html_content="""
            <div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;">
                <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;">
                    ğŸ“° {{ ds }} ì¼ì¼ ë‰´ìŠ¤ ë¦¬í¬íŠ¸
                </h2>

                <div style="background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 20px 0;">
                    <h3 style="color: #27ae60; margin-top: 0;">âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ</h3>
                    <p><strong>ì‹¤í–‰ ë‚ ì§œ:</strong> {{ ds }}</p>
                    <p><strong>ì‹¤í–‰ ì‹œê°„:</strong> {{ ts }}</p>
                </div>

                <div style="background-color: #fff; border: 1px solid #ddd; padding: 15px; border-radius: 5px; margin: 20px 0;">
                    <h3 style="color: #2980b9; margin-top: 0;">ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íë¦„</h3>
                    <ol style="line-height: 1.6;">
                        <li><strong>ì‹¤ì‹œê°„ ìˆ˜ì§‘:</strong> RSS â†’ Kafka â†’ Flink (AI ì²˜ë¦¬)</li>
                        <li><strong>ì €ì¥:</strong> PostgreSQL + HDFS (/user/realtime/)</li>
                        <li><strong>ë¶„ì„:</strong> Spark ë°°ì¹˜ ì‘ì—…</li>
                        <li><strong>ë¦¬í¬íŠ¸:</strong> PDF ìƒì„± ë° ì²¨ë¶€</li>
                        <li><strong>ì•„ì¹´ì´ë¸Œ:</strong> HDFS (/user/news_archive/{{ ds | replace("-", "/") }}/)</li>
                    </ol>
                </div>

                <div style="background-color: #e8f5e8; padding: 15px; border-radius: 5px; margin: 20px 0;">
                    <h3 style="color: #27ae60; margin-top: 0;">ğŸ“Š ì²˜ë¦¬ ê²°ê³¼</h3>
                    <p>â€¢ <strong>ë°ì´í„° ì†ŒìŠ¤:</strong> HDFS (/user/realtime/)</p>
                    <p>â€¢ <strong>ì•„ì¹´ì´ë¸Œ ìœ„ì¹˜:</strong> HDFS (/user/news_archive/{{ ds | replace("-", "/") }}/)</p>
                    <p>â€¢ <strong>AI ì²˜ë¦¬:</strong> ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œ, ë²¡í„° ì„ë² ë”©</p>
                    <p>â€¢ <strong>ë¶„ì„ ì—”ì§„:</strong> Apache Spark</p>
                    <p>â€¢ <strong>ë¦¬í¬íŠ¸ í˜•ì‹:</strong> PDF (ì²¨ë¶€ íŒŒì¼ ì°¸ì¡°)</p>
                </div>

                <div style="background-color: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 5px; margin: 20px 0;">
                    <h3 style="color: #d68910; margin-top: 0;">ğŸ“ ì²¨ë¶€ íŒŒì¼</h3>
                    <p>ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ëŠ” ì²¨ë¶€ëœ PDF íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                </div>

                <div style="text-align: center; margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd;">
                    <p style="color: #7f8c8d; font-size: 12px;">
                        ì´ ë¦¬í¬íŠ¸ëŠ” ë‰´ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
                        ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ë°ì´í„° íŒ€ìœ¼ë¡œ ì—°ë½ì£¼ì„¸ìš”.
                    </p>
                </div>
            </div>
            """,
            files=['/opt/airflow/output/daily_reports/{{ ds }}_report.pdf'],
            mime_subtype='mixed',
            doc_md="""
            ### ì´ë©”ì¼ ë¦¬í¬íŠ¸ ì „ì†¡
            - HTML í˜•ì‹ì˜ ìƒì„¸ ë¦¬í¬íŠ¸
            - PDF íŒŒì¼ ì²¨ë¶€
            - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
            """
        )

        # ë°°ì¹˜ ê·¸ë£¹ ë‚´ ì˜ì¡´ì„±
        check_hdfs_status >> submit_spark_job >> make_pdf_reports >> move_files_to_archive >> send_email_report

    # ========================================
    # ì „ì²´ DAG ì˜ì¡´ì„± ì„¤ì •
    # ========================================
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œëœ í›„ ë°°ì¹˜ ë¶„ì„ ì‹œì‘
    realtime_group >> batch_group