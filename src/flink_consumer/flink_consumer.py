"""
Flink ì»¨ìŠˆë¨¸ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
- Kafkaì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì†Œë¹„í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” Flink ì• í”Œë¦¬ì¼€ì´ì…˜
- ì „ì²˜ë¦¬, ì„ë² ë”© ìƒì„±, í‚¤ì›Œë“œ ì¶”ì¶œ, ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê¸°ëŠ¥
- JSON íŒŒì¼ì„ HDFSì— ì§ì ‘ ì €ì¥
- 30ì´ˆ idle íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ìë™ ì¢…ë£Œ
"""

import os
import sys
import json
import traceback
import requests
import time
import threading
from datetime import datetime
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.serialization import SimpleStringSchema
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.datastream.functions import MapFunction
from dotenv import load_dotenv

# í˜„ì¬ ê²½ë¡œ ì¶”ê°€ - ë„ì»¤ ì»¨í…Œì´ë„ˆ í™˜ê²½ì— ë§ê²Œ ì¡°ì •
sys.path.append('/opt/workspace')

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸ (í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ ì„í¬íŠ¸)
from db_handler import save_to_postgresql, test_database_connection
from preprocess_openai import (
    preprocess_content,
    transform_extract_keywords,
    transform_to_embedding,
    transform_classify_category
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def save_to_hdfs(data, filename):
    """WebHDFS APIë¥¼ ì‚¬ìš©í•˜ì—¬ HDFSì— JSON íŒŒì¼ ì €ì¥"""
    try:
        namenode_url = "http://namenode:9870"
        hdfs_path = f"/user/realtime/{filename}"
        
        # JSON ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        json_content = json.dumps(data, ensure_ascii=False, indent=2)
        
        # WebHDFS APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì—…ë¡œë“œ
        # 1ë‹¨ê³„: ì—…ë¡œë“œ URL ë°›ê¸°
        create_url = f"{namenode_url}/webhdfs/v1{hdfs_path}?op=CREATE&user.name=hadoop&overwrite=true"
        response = requests.put(create_url, allow_redirects=False, timeout=30)
        
        if response.status_code == 307:
            # 2ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œ
            upload_url = response.headers['Location']
            upload_response = requests.put(
                upload_url, 
                data=json_content.encode('utf-8'),
                headers={'Content-Type': 'application/json'},
                timeout=60
            )
            
            if upload_response.status_code == 201:
                print(f"HDFS ì €ì¥ ì„±ê³µ: {hdfs_path}")
                return True
            else:
                print(f"HDFS ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_response.status_code} - {upload_response.text}")
                return False
        else:
            print(f"HDFS ì—…ë¡œë“œ URL ìƒì„± ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"HDFS ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

class NewsProcessor(MapFunction):
    """Kafkaì—ì„œ ìˆ˜ì‹ í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë§µ í•¨ìˆ˜"""
    
    def __init__(self):
        self.processed_count = 0
        self.last_message_time = time.time()
        self.timeout_seconds = int(os.getenv("IDLE_TIMEOUT_SECONDS", "30"))  # ê¸°ë³¸ 30ì´ˆ
        
        print(f"ğŸ”§ {self.timeout_seconds}ì´ˆ idle íƒ€ì„ì•„ì›ƒ ì„¤ì •ë¨")
        
        # ê°„ë‹¨í•œ íƒ€ì„ì•„ì›ƒ ì²´ì»¤ ì‹œì‘
        self.start_timeout_checker()
    
    def start_timeout_checker(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ íƒ€ì„ì•„ì›ƒ ì²´í¬"""
        def timeout_checker():
            while True:
                time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
                idle_time = time.time() - self.last_message_time
                
                if idle_time > self.timeout_seconds:
                    print(f"â° {self.timeout_seconds}ì´ˆ ë™ì•ˆ ìƒˆ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"âœ… ëª¨ë“  ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {self.processed_count}ê°œ ì²˜ë¦¬ë¨)")
                    print("ğŸ¯ Flink Consumer ì •ìƒ ì¢…ë£Œ")
                    os._exit(0)
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸ (30ì´ˆë§ˆë‹¤)
                if int(idle_time) % 30 == 0 and idle_time > 0:
                    print(f"[INFO] ëŒ€ê¸° ì¤‘... (ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¡œë¶€í„° {int(idle_time)}ì´ˆ ê²½ê³¼, ì²˜ë¦¬ëœ ë©”ì‹œì§€: {self.processed_count}ê°œ)")
        
        timeout_thread = threading.Thread(target=timeout_checker)
        timeout_thread.daemon = True
        timeout_thread.start()
    
    def map(self, message):
        try:
            # ë©”ì‹œì§€ ìˆ˜ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_message_time = time.time()
            self.processed_count += 1
            
            print(f"[DEBUG] ìƒˆ ë©”ì‹œì§€ ìˆ˜ì‹  ({self.processed_count}ë²ˆì§¸): {message[:100]}...")
            
            # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            data = json.loads(message)
            
            # ë©”ì‹œì§€ ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            print(f"[Processing] {data.get('title', 'No title')}")
            
            # í•„ìš”í•œ í•„ë“œ ì¶”ì¶œ
            title = data.get('title', '')
            content = data.get('content', '')
            url = data.get('url', '')
            original_category = data.get('category', '')
            source = data.get('source', '')
            writer = data.get('writer', '')
            original_keywords = data.get('keywords', [])
            write_date = data.get('write_date','')
            
            # ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³€í™˜
            if content:
                print(f"[DEBUG] ì½˜í…ì¸  ì²˜ë¦¬ ì‹œì‘: {title[:50]}...")
                
                preprocessed_content = preprocess_content(content)
                
                # AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                ai_category = transform_classify_category(preprocessed_content)
                print(f"[DEBUG] AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: {ai_category}")
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ - ì›ë³¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ AIë¡œ ì¶”ì¶œ
                if not original_keywords:
                    keywords = transform_extract_keywords(preprocessed_content)
                else:
                    keywords = original_keywords
                print(f"[DEBUG] ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
                
                # ë²¡í„° ì„ë² ë”© ìƒì„±
                embedding = transform_to_embedding(preprocessed_content)
                print(f"[DEBUG] ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(embedding)})")
                
                # PostgreSQLì— ì €ì¥
                save_result = save_to_postgresql(
                    title=title,
                    content=preprocessed_content,
                    url=url,
                    original_category=original_category,
                    ai_category=ai_category,
                    keywords=keywords,
                    embedding=embedding,
                    source=source,
                    writer=writer
                )

                output_data = {
                    "title":title,
                    "content":preprocessed_content,
                    "url":url,
                    "original_category": original_category,
                    "ai_category": ai_category,
                    "keywords":keywords,
                    "embedding":embedding, 
                    "source":source,
                    "writer":writer, 
                    "publish_date":write_date
                }

                # HDFSì— ì €ì¥
                filename = f"news_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.json"
                hdfs_save_result = save_to_hdfs(output_data, filename)
                
                if hdfs_save_result:
                    print(f"HDFS ì €ì¥ ì™„ë£Œ: /user/realtime/{filename}")
                else:
                    print(f"HDFS ì €ì¥ ì‹¤íŒ¨: {filename}")

                print(f"[DEBUG] DB ì €ì¥ ê²°ê³¼: {save_result}")
                print(f"âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ ({self.processed_count}ë²ˆì§¸): {title}")
                return f"Successfully processed: {title}"

            else:
                print(f"[Skip] ë‚´ìš© ì—†ìŒ: {title}")
                return f"Skipped (empty content): {title}"
                
        except Exception as e:
            error_msg = f"Error processing message: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            return error_msg

def main():
    print("Flink ì»¨ìŠˆë¨¸ ì‹œì‘: ì´ˆê¸° ì„¤ì •")
    
    # ì„¤ì • ê°’ í™•ì¸
    timeout_seconds = int(os.getenv("IDLE_TIMEOUT_SECONDS", "60"))
    print(f"ğŸ”§ Idle íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
    if not test_database_connection():
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨. ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
    
    # Flink ì‹¤í–‰ í™˜ê²½ ì„¤ì •
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(1)  # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ì¤€ ì„¤ì •
    
    print("âœ… Flink í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    # Kafka connector JAR ë“±ë¡ - í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    kafka_jar = os.getenv("KAFKA_CONNECTOR_PATH", "/opt/flink/lib/flink-sql-connector-kafka-3.3.0-1.20.jar")
    
    print(f"ğŸ“¦ ì¹´í”„ì¹´ ì»¤ë„¥í„° ê²½ë¡œ: {kafka_jar}")
    env.add_jars(f"file://{kafka_jar}")
    
    print("âœ… Kafka ì»¤ë„¥í„° JAR ë“±ë¡ ì™„ë£Œ")
    
    # Kafka Consumer ì„¤ì • - ë„ì»¤ ë„¤íŠ¸ì›Œí¬ì— ë§ê²Œ ì„œë²„ ì£¼ì†Œ ë³€ê²½
    kafka_props = {
        'bootstrap.servers': 'kafka:9092',  # ë„ì»¤ ì»¨í…Œì´ë„ˆ ì´ë¦„ìœ¼ë¡œ í˜¸ìŠ¤íŠ¸ ì„¤ì •
        'group.id': 'flink_news_processor',
        'auto.offset.reset': 'earliest'  # ê°€ì¥ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¶€í„° ì½ê¸°
    }
    
    print("âœ… Kafka ì†ì„± ì„¤ì • ì™„ë£Œ")
    
    consumer = FlinkKafkaConsumer(
        topics='news',
        deserialization_schema=SimpleStringSchema(),
        properties=kafka_props
    )
    
    # ëª¨ë“  ë©”ì‹œì§€ ì½ë„ë¡ ì„¤ì •
    consumer.set_start_from_earliest()
    
    print("âœ… Kafka ì»¨ìŠˆë¨¸ ì„¤ì • ì™„ë£Œ")
    
    # Kafkaì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ 
    stream = env.add_source(consumer)
    
    print("âœ… ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼ ìƒì„±")
    
    # ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ì ìš©
    processed_stream = stream.map(NewsProcessor())
    
    # ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥ (ì˜µì…˜)
    processed_stream.print()
    
    print("âœ… ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼ ì„¤ì • ì™„ë£Œ")
    
    # Flink ì‘ì—… ì‹¤í–‰
    print("ğŸš€ Flink ì‘ì—… ì‹¤í–‰ ì‹œë„...")
    print(f"ğŸ“Š ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. {timeout_seconds}ì´ˆ idle íƒ€ì„ì•„ì›ƒì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("â° ìƒˆ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.")
    
    try:
        env.execute("News Data Processing and Storage")
        print("âœ… Flink ì‘ì—… ì •ìƒ ì‹¤í–‰")
    except Exception as e:
        print(f"âŒ Flink ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        print("ğŸ¯ Flink Consumer ì¢…ë£Œ")

if __name__ == "__main__":
    main()